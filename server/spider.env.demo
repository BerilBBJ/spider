DB_USER=${DB_USER}
DB_PASSWORD=${DB_PASSWORD}
DB_NAME=${DB_NAME}
DB_HOST=${DB_HOST}
DB_PORT=${DB_PORT}
DB_MAX_CONNECTIONS=10
DB_MIN_CONNECTIONS=0
SEARCH_DEPTH=1

TOR_HOST=$TOR_HOST
TOR_CONTROL_PORT=9077

NETWORK_MAX_CONNECTIONS=100
NETWORK_MIN_POOL_SIZE=700
NETWORK_MAX_POOL_SIZE=2000

PREPROCESSOR_MAX_PARALLEL=25
PREPROCESSOR_MAX_DB_CONNECTIONS=25
PREPROCESSOR_MAX_POOL_SIZE=2000
PREPROCESSOR_NUM_THREADS=2

CLASSIFIER_MIN_DF_FREQ=0.005
CLASSIFIER_QUANTILE=0.001
CLASSIFIER_LIMIT=10000
CLASSIFIER_NUM_THREADS=2

CLASSIFIER_SVM_TYPE=C_SVC
CLASSIFIER_COST=10
CLASSIFIER_KERNEL_TYPE=RBF
CLASSIFIER_NU=0.125
CLASSIFIER_GAMMA=38
CLASSIFIER_DEGREE=3
CLASSIFIER_R=0.5
CLASSIFIER_KFOLD=4
CLASSIFIER_NORMALIZE=False
CLASSIFIER_REDUCE=False
CLASSIFIER_RETAINED_VARIANCE=0.99
CLASSIFIER_EPS=0.001
CLASSIFIER_CACHE_SIZE=1024
CLASSIFIER_SHRINKING=True
CLASSIFIER_PROBABILITY=True
CLASSIFIER_LANGUAGE=all

LOG_LEVEL=silly # silly/debug/info/warn/error

SCRAPING_STRATEGY=random # new/prioritized/recursive/random/combined/inverse

LOG_LOCATION=/var/log # specify a directory that you are allowed to write to

# Set to true if you want to attach
# to a database on which already a
# spider is running. This enables multiple
# spiders to work on the same dataset
ATTACH=false
