DB_USER=${DB_USER}
DB_PASSWORD=${DB_PASSWORD}
DB_NAME=${DB_NAME}
DB_HOST=${DB_HOST}
DB_PORT=${DB_PORT}
DB_MAX_CONNECTIONS=10
DB_MIN_CONNECTIONS=0
SEARCH_DEPTH=1

TOR_HOST=$TOR_HOST
TOR_CONTROL_PORT=9077

NETWORK_MAX_CONNECTIONS=100
NETWORK_MIN_POOL_SIZE=700
NETWORK_MAX_POOL_SIZE=2000

PREPROCESSOR_MAX_PARALLEL=25
PREPROCESSOR_MAX_DB_CONNECTIONS=25
PREPROCESSOR_MAX_POOL_SIZE=2000
PREPROCESSOR_NUM_THREADS=2

CLASSIFIER_QUANTILE=0.001
CLASSIFIER_LIMIT=10000
CLASSIFIER_NUM_THREADS=2

LOG_LEVEL=silly # silly/debug/info/warn/error

SCRAPING_STRATEGY=random # new/prioritized/recursive/random/combined/inverse

LOG_LOCATION=/var/log # specify a directory that you are allowed to write to

# Set to true if you want to attach
# to a database on which already a
# spider is running. This enables multiple
# spiders to work on the same dataset
ATTACH=false
